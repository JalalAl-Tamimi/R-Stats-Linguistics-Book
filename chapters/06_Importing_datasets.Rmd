# Accessing textual datasets {#Text_Data}

# Loading packages

```{r}
## Use the code below to check if you have all required packages installed. If some are not installed already, the code below will install these. If you have all packages installed, then you could load them with the second code.
requiredPackages = c('tidyverse', 'tidytext', 'rvest', 'janeaustenr', 'proustr', 'textdata', 'gutenbergr', 'quanteda', 'readtext')
for(p in requiredPackages){
  if(!require(p,character.only = TRUE)) install.packages(p)
  library(p,character.only = TRUE)
}
```

# Where to find textual datasets?

There are various sources where one can easily find textual corpora. You can look here [link](https://github.com/EmilHvitfeldt/R-text-data). 

This session is inspired by data manipulations from the [Quanteda Tutorials](https://tutorials.quanteda.io/)

We have installed various packages which allow you to obtain textual data. Here are a few examples

## janeaustenr

### Look at books

This library includes 6 books by Jane Austen

```{r}
austen_books() %>% 
  glimpse()
```

### Summary

Books are named as: emma, mansfieldpark, northangerabbey, persuasion, prideprejudice and sensesensibility

We use `summary()` to get number of characters in each 

```{r}
austen_books() %>%
  summary()
```

### Import into `Global Environment`

Adding data from the first book into the `Global Environment`

```{r}
data(sensesensibility)
#data(prideprejudice)
#data(mansfieldpark)
#data(emma)
#data(northangerabbey)
#data(persuasion)
```


### Look into data

And we can get the top 60 rows from "sensesensibility"

```{r}
sensesensibility %>% 
  head(n = 60)
```


## proustr

### Look at books

This library includes 7 books written in French

```{r}
proust_books() %>% 
  glimpse()
```

### Summary

We use `summary()` to get number of characters in each 

```{r}
proust_books() %>%
  mutate(book = factor(book)) %>% 
  summary()
```

### Import into `Global Environment`

Adding data from the first book into the Global environment

```{r}
data(ducotedechezswann)
#data(alombredesjeunesfillesenfleurs)
#data(lecotedeguermantes)
#data(sodomeetgomorrhe)
#data(laprisonniere)
#data(albertinedisparue)
#data(letempretrouve)
```


### Look into data

And we can get the top 60 rows from the first one 

```{r}
ducotedechezswann %>%
  head(n = 60)
```

## gutenbergr

The gutenbergr package allows for search and download of public domain texts from [Project Gutenberg](https://www.gutenberg.org/)

To use gutenbergr you must know the Gutenberg id of the work you wish to analyse. A text search of the works can be done using the `gutenberg_metadata` function.

### Search for available work

```{r}
gutenberg_metadata
```

### Filter available text

We can filter only available text

```{r}
gutenberg_works(only_text = TRUE)
```


### Look at a specific work

Then we can search for a specific work

```{r}
gutenberg_works(title == "Wuthering Heights")
```


### Download specific work

Then use the `gutenberg_download()` function with the ID to download the book


```{r}
book_768 <- gutenberg_download(768)
```

### Summary

We use `summary()` to get number of characters in each 

```{r}
book_768 %>%
  summary()
```

### Look into data

And we can get the top 60 rows 

```{r}
book_768 %>%
  head(n = 60)
```


## textdata

The textdata package allows to find and download textual datasets. See this [link](https://emilhvitfeldt.github.io/textdata/) for details of datasets.

### Available datasets

```{r}
catalogue
```


```{r}
with(catalogue, split(name, type))
```

### Download datasets

We download the smallest datasets from `textdata`: 

```{r}
#AFINN-111 sentiment lexicon
lexicon_afinn()
```



```{r}
lexicon_afinn() %>% 
  group_by(factor(word)) %>% 
  summary()
```

### Look into data

```{r}
lexicon_afinn() %>% 
  head(n = 60)
```

## readtext

The readtext package comes with various datasets. We specify the path to where to find the datasets and upload them

```{r}
Data_Dir <- system.file("extdata/", package = "readtext")
```


### Inaugural Corpus USA

#### Importing data

```{r}
dat_inaug <- read.csv(paste0(Data_Dir, "/csv/inaugCorpus.csv"))
```


#### Checking structure

```{r}
dat_inaug %>% 
  str()
```



#### Unnest

```{r}
dat_inaug %>% 
  unnest()
```





### Universal Declaration of Human Rights

We import multiple files containing the Universal Declaration of Human Rights in 13 languages. There are 13 different textfiles


#### Importing data

```{r}
dat_udhr <- readtext(paste0(Data_Dir, "/txt/UDHR/*"),
                      docvarsfrom = "filenames", 
                      docvarnames = c("document", "language"))
```


#### Checking structure

```{r}
dat_udhr %>% 
  str()
```


#### Unnest

```{r}
dat_udhr %>% 
  unnest()
```

### Twitter data

We the twitter.json data accessed from [here](https://github.com/quanteda/tutorials.quanteda.io/blob/master/content/data/twitter.json). This is a JSON file (.json) downloaded from the Twitter stream API.

#### Importing data

```{r}
dat_twitter <- readtext("data/twitter.json", source = "twitter")
```


#### Checking structure

```{r}
dat_twitter %>% 
  str()
```



#### Unnest

```{r}
dat_twitter %>% 
  unnest()
```








### Converting from a PDF file

We can also import data in a PDF format and obtain details from file name. 

#### Importing data

```{r}
dat_udhr_PDF <- readtext(paste0(Data_Dir, "/pdf/UDHR/*.pdf"), 
                      docvarsfrom = "filenames", 
                      docvarnames = c("document", "language"),
                      sep = "_")
```


#### Check encoding

```{r}
Encoding(dat_udhr_PDF$text)

```

#### Checking structure

```{r}
dat_udhr_PDF %>% 
  str()
```



#### Unnest

```{r}
dat_udhr_PDF %>% 
  unnest()
```


### Different encodings

We look into data with different encoding. This is important as the type of data you will generate can be of different encodings.

#### Temp path

```{r}
path_temp <- tempdir()
unzip(system.file("extdata", "data_files_encodedtexts.zip", package = "readtext"), exdir = path_temp)
```

#### Importing data

We use regular expressions to search for all files starting with "Indian_" or "UDHR_" and containing any characters, up to the ending with ".text"

```{r}
filename <- list.files(path_temp, "^(Indian|UDHR_).*\\.txt$")
head(filename)
```

#### Export encoding

We use various functions to delete `.txt` at the end of each file name and then we split each string as a function of `_` and obtain the third row in each list of item. 

```{r}
filename <- filename %>% 
  str_replace(".txt$", "")
encoding <- purrr::map(str_split(filename, "_"), 3)
head(encoding)
```

We feed the encoding to `readtext()` to convert various character encodings into UTF-8. 

```{r}
dat_txt <- readtext(paste0(Data_Dir, "/data_files_encodedtexts.zip"), 
                     encoding = encoding,
                     docvarsfrom = "filenames", 
                     docvarnames = c("document", "language", "input_encoding"))
```



```{r}
dat_txt %>% 
  unnest()
```



## Webscrapping

We use the `rvest` package to obtain data from a specific URL. See [here](https://ladal.edu.au/webcrawling.html) for advanced webscrapping. Look at this [link](https://jakobtures.github.io/web-scraping/index.html) as well for a more straightforward way.

### A single webpage

#### Read_html

```{r}
web_page <- rvest::read_html("https://www.tidyverse.org/packages/")
web_page
```

Because the downloaded file contains a unnecessary information. We process the data to extract only the text from the webpage.

#### Extract headline

```{r}
header_web_page <- web_page %>%
  # extract paragraphs
  rvest::html_nodes("h1") %>%
  # extract text
  rvest::html_text() 
head(header_web_page)
```



#### Extract text

```{r}
web_page_txt <- web_page %>%
  # extract paragraphs
  rvest::html_nodes("p") %>%
  # extract text
  rvest::html_text()
head(web_page_txt)
```


### Multiple webpages



#### Read_html

```{r}
website <- "https://www.tidyverse.org/packages/" %>% 
  rvest::read_html()
website
```

```{r}
a_elements <- website %>% 
  rvest::html_elements(css = "div.package > a")
a_elements
```


#### Extract headline

```{r}
links <- a_elements %>%
  rvest::html_attr(name = "href")
links
```



#### Extract subpages

```{r}
pages <- links %>% 
  map(rvest::read_html)
pages
```


The structure seems to be similar across all pages

```{r}
pages %>% 
  map(rvest::html_element, css = "a.navbar-brand") %>% 
  map_chr(rvest::html_text)

```

and extracting version number

```{r}
pages %>% 
  map(rvest::html_element, css = "small.nav-text.text-muted.me-auto") %>% 
  map_chr(rvest::html_text)
```


#### Extract text

and we can also add all into a tibble

```{r}
pages_table <- tibble(
  name = pages %>% 
    map(rvest::html_element, css = "a.navbar-brand") %>% 
    map_chr(rvest::html_text),
  version = pages %>% 
    map(rvest::html_element, css = "small.nav-text.text-muted.me-auto") %>% 
    map_chr(rvest::html_text),
  CRAN = pages %>% 
    map(rvest::html_element, css = "ul.list-unstyled > li:nth-child(1) > a") %>% 
    map_chr(rvest::html_attr, name = "href"),
  Learn = pages %>% 
    map(rvest::html_element, css = "ul.list-unstyled > li:nth-child(4) > a") %>% 
    map_chr(rvest::html_attr, name = "href"), 
  text = pages %>%
    map(rvest::html_element,  css = "body") %>%
    map_chr(rvest::html_text2)
)
pages_table
```


# session info

```{r}
sessionInfo()
```
