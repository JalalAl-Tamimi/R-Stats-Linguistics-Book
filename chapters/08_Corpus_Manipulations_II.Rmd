# Advanced manipulations of corpora {#Corpus_Advanced}

# Loading packages

```{r}
# Use the code below to check if you have all required packages installed. If some are not installed already, the code below will install these. If you have all packages installed, then you could load them with the second code.
requiredPackages = c('tidyverse', 'tidytext', 'rvest', 'janeaustenr', 'proustr', 'textdata', 'gutenbergr', 'quanteda', 'readtext', 'tm', 'SnowballC', 'stopwords', 'quanteda.textplots', 'udpipe', 'textplot', 'ggraph')
for(p in requiredPackages){
  if(!require(p,character.only = TRUE)) install.packages(p)
  library(p,character.only = TRUE)
}
```

There are various sources where one can easily find textual corpora.
You can look here [link](https://github.com/EmilHvitfeldt/R-text-data).

This session is inspired by data manipulations from the [Quanteda Tutorials](https://tutorials.quanteda.io/)

# janeaustenr

## Import into `Global Environment`

Adding data from the first book into the `Global Environment`

```{r}
data(sensesensibility)
#data(prideprejudice)
#data(mansfieldpark)
#data(emma)
#data(northangerabbey)
#data(persuasion)
```

## Look into data

And we can get the top 60 rows from "sensesensibility"

```{r}
sensesensibility %>% 
  head(n = 60)
```

## Transform to a dataframe

```{r}
sensesensibility_DF <- sensesensibility %>% 
  data.frame()
sensesensibility_DF
sensesensibility_DF <- sensesensibility_DF[-c(1:12),]
sensesensibility_DF
```

## Create a corpus

```{r}
sensesensibility_corpus <- corpus(sensesensibility_DF)
print(sensesensibility_corpus)
```

### Summary

```{r}
summary(sensesensibility_corpus, 10)
```

### Accessing parts of corpus

```{r}
ndoc(sensesensibility_corpus)
sensesensibility_corpus[[1]]
```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
sensesensibility_corpus_tok <- tokens(sensesensibility_corpus)
sensesensibility_corpus_tok
```

#### Without punctuations

```{r}
sensesensibility_corpus_tok_no_punct <- tokens(sensesensibility_corpus, remove_punct = TRUE)
sensesensibility_corpus_tok_no_punct
```

### Compound words

Compound words are multi-word expressions that are relevant for our analysis.
We already found these based the `kwic` function

#### `kwic` Phrase

```{r}
sensesensibility_corpus_tok_no_punct_phrase <- kwic(sensesensibility_corpus_tok_no_punct, pattern =  phrase("the house"), window = 6)
head(sensesensibility_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

However, these are computed as individual words not compound words.
To do so, we can use the following code.
As you see the phrase we were interested in is now added as compound words

```{r}
sensesensibility_corpus_tok_no_punct_comp <- tokens_compound(sensesensibility_corpus_tok_no_punct, pattern = phrase("the house"))
sensesensibility_corpus_tok_no_punct_comp_kwic <- kwic(sensesensibility_corpus_tok_no_punct_comp, pattern = phrase("the_house"))
head(sensesensibility_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
sensesensibility_corpus_tok_no_punct_ngram <- tokens_ngrams(sensesensibility_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(sensesensibility_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(sensesensibility_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
sensesensibility_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(sensesensibility_corpus_tok_no_punct, n = 2:4, skip = 1:2) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(sensesensibility_corpus_tok_no_punct_ngram_skip, 10)

# Last 10 rows
tail(sensesensibility_corpus_tok_no_punct_ngram_skip, 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_sensesensibility <- dictionary(list(large = c("large", "big"),
                        property = c("property", "house"),
                        good = c("good", "great")))
print(dict_sensesensibility)
```

#### Token lookup

```{r}
sensesensibility_corpus_tok_no_punct_dict_toks <- tokens_lookup(sensesensibility_corpus_tok_no_punct, dictionary = dict_sensesensibility)
print(sensesensibility_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(sensesensibility_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

This section borrows many features from [here](https://ladal.edu.au/tagging.html).
Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

![](PoS-tag.png){width="700"}


#### Download and load language model

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

```{r}
file_to_check <- "models/english-ewt-ud-2.5-191206.udpipe"

if (file.exists(file = file_to_check)){
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}else{
  m_english <- udpipe_download_model(model_dir = "models/", language = "english-ewt")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}
```


#### Tokenise, tag, dependency parsing

```{r}
sensesensibility_corpus_tok_no_punct_anndf <- udpipe_annotate(m_english, x = sensesensibility_corpus_tok_no_punct[[1]]) %>%
  as.data.frame() 
# inspect
head(sensesensibility_corpus_tok_no_punct_anndf, 10)
```

#### Dependency parsing

```{r}
# parse text
sensesensibility_corpus_sent <- udpipe_annotate(m_english, x = sensesensibility_corpus[[1]]) %>%
  as.data.frame()
# inspect
head(sensesensibility_corpus_sent)
```

```{r}
sensesensibility_corpus_sent_dplot <- textplot_dependencyparser(sensesensibility_corpus_sent, size = 3) 
# show plot
sensesensibility_corpus_sent_dplot
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

```{r}
sensesensibility_corpus_dfmat <- dfm(sensesensibility_corpus_tok_no_punct)
sensesensibility_corpus_dfmat_trim <- dfm_trim(sensesensibility_corpus_dfmat, min_termfreq = 500)

topfeatures_sensesensibility_corpus <- topfeatures(sensesensibility_corpus_dfmat_trim)
topfeatures_sensesensibility_corpus
nfeat(sensesensibility_corpus_dfmat_trim)
```

#### Features co-occurrences

```{r}
sensesensibility_corpus_fcmat <- fcm(sensesensibility_corpus_dfmat_trim)
sensesensibility_corpus_fcmat
```

# proustr

## Import into `Global Environment`

Adding data from the first book into the Global environment

```{r}
data(ducotedechezswann)
#data(alombredesjeunesfillesenfleurs)
#data(lecotedeguermantes)
#data(sodomeetgomorrhe)
#data(laprisonniere)
#data(albertinedisparue)
#data(letempretrouve)
```

## Look into data

And we can get the top 60 rows from the first one

```{r}
ducotedechezswann %>%
  head(n = 60)
```


## Create a corpus

```{r}
ducotedechezswann_corpus <- corpus(ducotedechezswann, text_field = "text")
print(ducotedechezswann_corpus)
```

### Summary

```{r}
summary(ducotedechezswann_corpus, 10)
```

### Accessing parts of corpus

```{r}
ducotedechezswann_corpus[[1]]
```

### Document-level information

```{r}
head(docvars(ducotedechezswann_corpus))

```

### Unique variable names (for volume)

```{r}
unique(docvars(ducotedechezswann_corpus, field = "volume"))

```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
ducotedechezswann_corpus_tok <- tokens(ducotedechezswann_corpus)
ducotedechezswann_corpus_tok
```

#### Without punctuations

```{r}
ducotedechezswann_corpus_tok_no_punct <- tokens(ducotedechezswann_corpus, remove_punct = TRUE)
ducotedechezswann_corpus_tok_no_punct
```

### Compound words

#### `kwic` Phrase

```{r}
ducotedechezswann_corpus_tok_no_punct_phrase <- kwic(ducotedechezswann_corpus_tok_no_punct, pattern =  phrase("un homme"), window = 6)
head(ducotedechezswann_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

```{r}
ducotedechezswann_corpus_tok_no_punct_comp <- tokens_compound(ducotedechezswann_corpus_tok_no_punct, pattern = phrase("un homme"))
ducotedechezswann_corpus_tok_no_punct_comp_kwic <- kwic(ducotedechezswann_corpus_tok_no_punct_comp, pattern = phrase("un_homme"))
head(ducotedechezswann_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
ducotedechezswann_corpus_tok_no_punct_ngram <- tokens_ngrams(ducotedechezswann_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(ducotedechezswann_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(ducotedechezswann_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
ducotedechezswann_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(ducotedechezswann_corpus_tok_no_punct, n = 2:4, skip = 1:2) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(ducotedechezswann_corpus_tok_no_punct_ngram_skip, 10)

# Last 10 rows
tail(ducotedechezswann_corpus_tok_no_punct_ngram_skip, 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_ducotedechezswann <- dictionary(list(famille = c("grand-père", "femme", "homme", "oncle"),
                        dormir = c("rendormais", "dors", "dort")))
print(dict_ducotedechezswann)
```

#### Token lookup

```{r}
ducotedechezswann_corpus_tok_no_punct_dict_toks <- tokens_lookup(ducotedechezswann_corpus_tok_no_punct, dictionary = dict_ducotedechezswann)
print(ducotedechezswann_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(ducotedechezswann_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

#### Download and load language model

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

```{r}
file_to_check <- "models/french-partut-ud-2.5-191206.udpipe"

if (file.exists(file = file_to_check)){
  m_french <- udpipe_load_model(file = "models/french-partut-ud-2.5-191206.udpipe")
}else{
  m_french <- udpipe_download_model(model_dir = "models/", language = "french-partut")
  m_french <- udpipe_load_model(file = "models/french-partut-ud-2.5-191206.udpipe")
}
```

#### Tokenise, tag, dependency parsing

We use the already tokenised text, with no punctuations.

```{r}
ducotedechezswann_anndf <- udpipe_annotate(m_french, x = ducotedechezswann_corpus_tok_no_punct[[1]]) %>%
  as.data.frame() 
# inspect
head(ducotedechezswann_anndf, 10)
```

#### Dependency parsing

```{r}
# parse text
ducotedechezswann_corpus_sent <- udpipe_annotate(m_french, x = ducotedechezswann_corpus[[1]]) %>%
  as.data.frame()
# inspect
head(ducotedechezswann_corpus_sent)
```

```{r}
ducotedechezswann_corpus_sent_dplot <- textplot_dependencyparser(ducotedechezswann_corpus_sent, size = 3) 
# show plot
ducotedechezswann_corpus_sent_dplot
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

```{r}
ducotedechezswann_corpus_dfmat <- dfm(ducotedechezswann_corpus_tok_no_punct)
ducotedechezswann_corpus_dfmat_trim <- dfm_trim(ducotedechezswann_corpus_dfmat, min_termfreq = 500)

topfeatures_ducotedechezswann_corpus <- topfeatures(ducotedechezswann_corpus_dfmat_trim)
topfeatures_ducotedechezswann_corpus
nfeat(ducotedechezswann_corpus_dfmat_trim)
```

#### Features co-occurrences

```{r}
ducotedechezswann_corpus_fcmat <- fcm(ducotedechezswann_corpus_dfmat_trim)
ducotedechezswann_corpus_fcmat
```


# Inaugural Corpus USA

The readtext package comes with various datasets.
We specify the path to where to find the datasets and upload them

```{r}
Data_Dir <- system.file("extdata/", package = "readtext")
```

## Importing data

```{r}
dat_inaug <- read.csv(paste0(Data_Dir, "/csv/inaugCorpus.csv"))
```

## Create a corpus

```{r}
dat_inaug_corpus <- corpus(dat_inaug, text_field = "texts")
print(dat_inaug_corpus)
```

### Summary

```{r}
summary(dat_inaug_corpus, 10)
```

### Editing docnames

```{r}
docid <- paste(dat_inaug$Year, 
               dat_inaug$FirstName, 
               dat_inaug$President, sep = " ")
docnames(dat_inaug_corpus) <- docid
print(dat_inaug_corpus)
```

### Accessing parts of corpus

```{r}
dat_inaug_corpus[[1]]
```

```{r}
dat_inaug_corpus[["1789 George Washington"]]
```

### Document-level information

```{r}
head(docvars(dat_inaug_corpus))

```

### Unique variable names (for volume)

```{r}
unique(docvars(dat_inaug_corpus, field = "Year"))

```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
dat_inaug_corpus_tok <- tokens(dat_inaug_corpus)
dat_inaug_corpus_tok
```

#### Without punctuations

```{r}
dat_inaug_corpus_tok_no_punct <- tokens(dat_inaug_corpus, remove_punct = TRUE)
dat_inaug_corpus_tok_no_punct
```

### Compound words

#### `kwic` Phrase

```{r}
dat_inaug_corpus_tok_no_punct_phrase <- kwic(dat_inaug_corpus_tok_no_punct, pattern =  phrase("the Constitution"), window = 6)
head(dat_inaug_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

```{r}
dat_inaug_corpus_tok_no_punct_comp <- tokens_compound(dat_inaug_corpus_tok_no_punct, pattern = phrase("the Constitution"))
dat_inaug_corpus_tok_no_punct_comp_kwic <- kwic(dat_inaug_corpus_tok_no_punct_comp, pattern = phrase("the_Constitution"))
head(dat_inaug_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
dat_inaug_corpus_tok_no_punct_ngram <- tokens_ngrams(dat_inaug_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(dat_inaug_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(dat_inaug_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
dat_inaug_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(dat_inaug_corpus_tok_no_punct, n = 2:4, skip = 1:2) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(dat_inaug_corpus_tok_no_punct_ngram_skip, 10)

# Last 10 rows
tail(dat_inaug_corpus_tok_no_punct_ngram_skip, 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_dat_inaug <- dictionary(list(Population = c("Citizens*", "people"),
                        upper_house = c("Representatives", "senat")))
print(dict_dat_inaug)
```

#### Token lookup

```{r}
dat_inaug_corpus_tok_no_punct_dict_toks <- tokens_lookup(dat_inaug_corpus_tok_no_punct, dictionary = dict_dat_inaug)
print(dat_inaug_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(dat_inaug_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

#### Download and load language model

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

```{r}
file_to_check <- "models/english-ewt-ud-2.5-191206.udpipe"

if (file.exists(file = file_to_check)){
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}else{
  m_english <- udpipe_download_model(model_dir = "models/", language = "english-ewt")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}
```


#### Tokenise, tag, dependency parsing

We use the already tokenised text, with no punctuations.

```{r}
dat_inaug_anndf <- udpipe_annotate(m_english, x = dat_inaug_corpus_tok_no_punct[[1]]) %>%
  as.data.frame() 
# inspect
head(dat_inaug_anndf, 10)
```

#### Dependency parsing

```{r}
# parse text
dat_inaug_corpus_sent <- udpipe_annotate(m_english, x = dat_inaug_corpus[[1]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent)
```

```{r}
dat_inaug_corpus_sent_dplot <- textplot_dependencyparser(dat_inaug_corpus_sent, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

```{r}
dat_inaug_corpus_dfmat <- dfm(dat_inaug_corpus_tok_no_punct)
dat_inaug_corpus_dfmat_trim <- dfm_trim(dat_inaug_corpus_dfmat, min_termfreq = 50)

topfeatures_dat_inaug_corpus <- topfeatures(dat_inaug_corpus_dfmat_trim)
topfeatures_dat_inaug_corpus
nfeat(dat_inaug_corpus_dfmat_trim)
```

#### Features co-occurrences

```{r}
dat_inaug_corpus_fcmat <- fcm(dat_inaug_corpus_dfmat_trim)
dat_inaug_corpus_fcmat
```

# Universal Declaration of Human Rights

We import multiple files containing the Universal Declaration of Human Rights in 13 languages.
There are 13 different textfiles

## Importing data

```{r}
dat_udhr <- readtext(paste0(Data_Dir, "/txt/UDHR/*"),
                      docvarsfrom = "filenames", 
                      docvarnames = c("document", "language"))
```


## Create a corpus

```{r}
dat_udhr_corpus <- corpus(dat_udhr)
print(dat_udhr_corpus)
```

### Summary

```{r}
summary(dat_udhr_corpus, 13)
```

### Accessing parts of corpus

```{r}
dat_udhr_corpus[[4]]
```

### Document-level information

```{r}
head(docvars(dat_udhr_corpus))

```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
dat_udhr_corpus_tok <- tokens(dat_udhr_corpus)
dat_udhr_corpus_tok
```

#### Without punctuations

```{r}
dat_udhr_corpus_tok_no_punct <- tokens(dat_udhr_corpus, remove_punct = TRUE)
dat_udhr_corpus_tok_no_punct
```

### Compound words

#### `kwic` Phrase

```{r}
dat_udhr_corpus_tok_no_punct_phrase <- kwic(dat_udhr_corpus_tok_no_punct, pattern =  phrase("Human Rights"), window = 6)
head(dat_udhr_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

```{r}
dat_udhr_corpus_tok_no_punct_comp <- tokens_compound(dat_udhr_corpus_tok_no_punct, pattern = phrase("Human Rights"))
dat_udhr_corpus_tok_no_punct_comp_kwic <- kwic(dat_udhr_corpus_tok_no_punct_comp, pattern = phrase("Human_Rights"))
head(dat_udhr_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
dat_udhr_corpus_tok_no_punct_ngram <- tokens_ngrams(dat_udhr_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(dat_udhr_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(dat_udhr_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
dat_udhr_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(dat_udhr_corpus_tok_no_punct, n = 2:4, skip = 1:2) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(dat_udhr_corpus_tok_no_punct_ngram_skip, 10)

# Last 10 rows
tail(dat_udhr_corpus_tok_no_punct_ngram_skip, 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_dat_udhr <- dictionary(list(Human = c("Human*", "people"),
                        bad_aspects = c("tyranny", "barbarous", "oppression", "disregard")))
print(dict_dat_udhr)
```

#### Token lookup

```{r}
dat_udhr_corpus_tok_no_punct_dict_toks <- tokens_lookup(dat_udhr_corpus_tok_no_punct, dictionary = dict_dat_udhr)
print(dat_udhr_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(dat_udhr_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

#### Download and load language models

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

Because we have 13 languages in our document, we can download the language models for all of the ones available.

These are the models available: - chinese: chinese-gsd - czech: czech-cac - danish: danish-ddt - english: english-ewt - french: french-partut - georgian: NO language model!
- greek: greek-gdt - hungarian: hungarian-szeged - icelandic: NO language model!
- irish: irish-idt - japanese: japanese-gsd - russian: russian-gsd - vietnamese: vietnamese-vtb

With the function below, we check if the file exists and if this is the case, we load it; otherwise, we download it and load it later on

```{r}
file_to_check <- list("models/chinese-gsd-ud-2.5-191206.udpipe", "models/czech-cac-ud-2.5-191206.udpipe", 
  "models/danish-ddt-ud-2.5-191206.udpipe", "models/english-ewt-ud-2.5-191206.udpipe",
  "models/french-partut-ud-2.5-191206.udpipe", "models/greek-gdt-ud-2.5-191206.udpipe", 
  "models/hungarian-szeged-ud-2.5-191206.udpipe", "models/irish-idt-ud-2.5-191206.udpipe", 
  "models/japanese-gsd-ud-2.5-191206.udpipe", "models/russian-gsd-ud-2.5-191206.udpipe", 
  "models/vietnamese-vtb-ud-2.5-191206.udpipe")


for (x in file_to_check){
if (file.exists(file = x)){
  m_chinese <- udpipe_load_model(file = "models/chinese-gsd-ud-2.5-191206.udpipe")
  m_czech <- udpipe_load_model(file = "models/czech-cac-ud-2.5-191206.udpipe")
  m_danish <- udpipe_load_model(file = "models/danish-ddt-ud-2.5-191206.udpipe")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
  m_french <- udpipe_load_model(file = "models/french-partut-ud-2.5-191206.udpipe")
  m_greek <- udpipe_load_model(file = "models/greek-gdt-ud-2.5-191206.udpipe")
  m_hungarian <- udpipe_load_model(file = "models/hungarian-szeged-ud-2.5-191206.udpipe")
  m_irish <- udpipe_load_model(file = "models/irish-idt-ud-2.5-191206.udpipe")
  m_japanese <- udpipe_load_model(file = "models/japanese-gsd-ud-2.5-191206.udpipe")
  m_russian <- udpipe_load_model(file = "models/russian-gsd-ud-2.5-191206.udpipe")
  m_vietnamese <- udpipe_load_model(file = "models/vietnamese-vtb-ud-2.5-191206.udpipe")
}else{
  m_chinese <- udpipe_download_model(model_dir = "models/", language = "chinese-gsd")
  m_chinese <- udpipe_load_model(file = "models/chinese-gsd-ud-2.5-191206.udpipe")
  m_czech <- udpipe_download_model(model_dir = "models/", language = "czech-cac")
  m_czech <- udpipe_load_model(file = "models/czech-cac-ud-2.5-191206.udpipe")
  m_danish <- udpipe_download_model(model_dir = "models/", language = "danish-ddt")
  m_danish <- udpipe_load_model(file = "models/danish-ddt-ud-2.5-191206.udpipe")
  m_english <- udpipe_download_model(model_dir = "models/", language = "english-ewt")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
  m_french <- udpipe_download_model(model_dir = "models/", language = "french-partut")
  m_french <- udpipe_load_model(file = "models/french-partut-ud-2.5-191206.udpipe")
  m_greek <- udpipe_download_model(model_dir = "models/", language = "greek-gdt")
  m_greek <- udpipe_load_model(file = "models/greek-gdt-ud-2.5-191206.udpipe")
  m_hungarian <- udpipe_download_model(model_dir = "models/", language = "hungarian-szeged")
  m_hungarian <- udpipe_load_model(file = "models/hungarian-szeged-ud-2.5-191206.udpipe")
  m_irish <- udpipe_download_model(model_dir = "models/", language = "irish-idt")
  m_irish <- udpipe_load_model(file = "models/irish-idt-ud-2.5-191206.udpipe")
  m_japanese <- udpipe_download_model(model_dir = "models/", language = "japanese-gsd")
  m_japanese <- udpipe_load_model(file = "models/japanese-gsd-ud-2.5-191206.udpipe")
  m_russian <- udpipe_download_model(model_dir = "models/", language = "russian-gsd")
  m_russian <- udpipe_load_model(file = "models/russian-gsd-ud-2.5-191206.udpipe")
  m_vietnamese <- udpipe_download_model(model_dir = "models/", language = "vietnamese-vtb")
  m_vietnamese <- udpipe_load_model(file = "models/vietnamese-vtb-ud-2.5-191206.udpipe")
}
}
```


#### Tokenise, tag, dependency parsing

We use the already tokenised text, with no punctuations.

##### [**Chinese**]{.smallcaps}

```{r}
dat_udhr_anndf_chinese <- udpipe_annotate(m_chinese, x = dat_udhr_corpus_tok_no_punct[[1]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_chinese, 10)
```

##### [**Czech**]{.smallcaps}

```{r}
dat_udhr_anndf_czech <- udpipe_annotate(m_czech, x = dat_udhr_corpus_tok_no_punct[[2]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_czech, 10)
```

##### [**Danish**]{.smallcaps}

```{r}
dat_udhr_anndf_danish <- udpipe_annotate(m_danish, x = dat_udhr_corpus_tok_no_punct[[3]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_danish, 10)
```

##### [**English**]{.smallcaps}

```{r}
dat_udhr_anndf_english <- udpipe_annotate(m_english, x = dat_udhr_corpus_tok_no_punct[[4]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_english, 10)
```

##### [**French**]{.smallcaps}

```{r}
dat_udhr_anndf_french <- udpipe_annotate(m_french, x = dat_udhr_corpus_tok_no_punct[[5]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_french, 10)
```

##### [**Greek**]{.smallcaps}

```{r}
dat_udhr_anndf_greek <- udpipe_annotate(m_greek, x = dat_udhr_corpus_tok_no_punct[[7]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_greek, 10)
```

##### [**Hungarian**]{.smallcaps}

```{r}
dat_udhr_anndf_hungarian <- udpipe_annotate(m_hungarian, x = dat_udhr_corpus_tok_no_punct[[8]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_hungarian, 10)
```

##### [**Irish**]{.smallcaps}

```{r}
dat_udhr_anndf_irish <- udpipe_annotate(m_irish, x = dat_udhr_corpus_tok_no_punct[[10]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_irish, 10)
```

##### [**Japanese**]{.smallcaps}

```{r}
dat_udhr_anndf_japanese <- udpipe_annotate(m_japanese, x = dat_udhr_corpus_tok_no_punct[[11]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_japanese, 10)
```

##### [**Russian**]{.smallcaps}

```{r}
dat_udhr_anndf_russian <- udpipe_annotate(m_russian, x = dat_udhr_corpus_tok_no_punct[[12]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_russian, 10)
```

##### [**Vietnamese**]{.smallcaps}

```{r}
dat_udhr_anndf_vietnamese <- udpipe_annotate(m_vietnamese, x = dat_udhr_corpus_tok_no_punct[[13]]) %>%
  as.data.frame() 
# inspect
head(dat_udhr_anndf_vietnamese, 10)
```


#### Dependency parsing

Below, we look into the data and identify when there is a punctuation mark and use that as our limit for generating the dependency parsing plot.
We start by obtaining the corpus for hte full textfile and then filter out.

##### [**Chinese**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_chinese <- udpipe_annotate(m_chinese, x = dat_udhr_corpus[[1]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_chinese)
```


```{r}
dat_inaug_corpus_sent_chinese_short <- dat_inaug_corpus_sent_chinese[1:32,]
dat_inaug_corpus_sent_dplot_chinese <- textplot_dependencyparser(dat_inaug_corpus_sent_chinese_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_chinese
```

##### [**Czech**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_czech <- udpipe_annotate(m_czech, x = dat_udhr_corpus[[2]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_czech)
```

```{r}
dat_inaug_corpus_sent_czech_short <- dat_inaug_corpus_sent_czech[1:9,]
dat_inaug_corpus_sent_dplot_czech <- textplot_dependencyparser(dat_inaug_corpus_sent_czech_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_czech
```

##### [**Danish**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_danish <- udpipe_annotate(m_danish, x = dat_udhr_corpus[[3]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_danish)
```

```{r}
dat_inaug_corpus_sent_danish_short <- dat_inaug_corpus_sent_danish[1:14,]
dat_inaug_corpus_sent_dplot_danish <- textplot_dependencyparser(dat_inaug_corpus_sent_danish_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_danish
```

##### [**English**]{.smallcaps}

```{r}
# parse text
dat_udhr_corpus_english <- dat_udhr_corpus[[4]]
dat_udhr_corpus_english_short <- str_extract(dat_udhr_corpus_english, "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world")
dat_inaug_corpus_sent_english <- udpipe_annotate(m_english, x = dat_udhr_corpus_english_short) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_english)
```

```{r}
dat_inaug_corpus_sent_dplot_english <- textplot_dependencyparser(dat_inaug_corpus_sent_english, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_english
```

##### [**French**]{.smallcaps}

```{r}
# parse text
dat_udhr_corpus_french <- dat_udhr_corpus[[5]]
dat_udhr_corpus_french_short <- str_extract(dat_udhr_corpus_french, "Considérant que la reconnaissance de la dignité inhérente à tous les membres de la famille humaine et de leurs droits égaux et inaliénables constitue le fondement de la liberté, de la justice et de la paix dans le monde")
dat_inaug_corpus_sent_french <- udpipe_annotate(m_french, x = dat_udhr_corpus_french_short) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_french)
```

```{r}
dat_inaug_corpus_sent_dplot_french <- textplot_dependencyparser(dat_inaug_corpus_sent_french, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_french
```

##### [**Greek**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_greek <- udpipe_annotate(m_greek, x = dat_udhr_corpus[[7]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_greek)
```

```{r}
dat_inaug_corpus_sent_greek_short <- dat_inaug_corpus_sent_greek[4:51,]
dat_inaug_corpus_sent_dplot_greek <- textplot_dependencyparser(dat_inaug_corpus_sent_greek_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_greek
```

##### [**Hungarian**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_hungarian <- udpipe_annotate(m_hungarian, x = dat_udhr_corpus[[8]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_hungarian)
```

```{r}
dat_inaug_corpus_sent_hungarian_short <- dat_inaug_corpus_sent_hungarian[1:36,]
dat_inaug_corpus_sent_dplot_hungarian <- textplot_dependencyparser(dat_inaug_corpus_sent_hungarian_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_hungarian
```

##### [**Irish**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_irish <- udpipe_annotate(m_irish, x = dat_udhr_corpus[[10]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_irish)
```

```{r}
dat_inaug_corpus_sent_irish_short <- dat_inaug_corpus_sent_irish[1:37,]
dat_inaug_corpus_sent_dplot_irish <- textplot_dependencyparser(dat_inaug_corpus_sent_irish_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_irish
```

##### [**Japanese**]{.smallcaps}

```{r}
# parse text
dat_udhr_corpus_japanese <- dat_udhr_corpus[[11]]
dat_udhr_corpus_japanese_short <- str_extract(dat_udhr_corpus_japanese, "人類社会のすべての構成員の固有の尊厳と平等で譲ることのできない権利とを承 認することは、世界における自由、正義及び平和の基礎であるので")
dat_inaug_corpus_sent_japanese <- udpipe_annotate(m_japanese, x = dat_udhr_corpus_japanese_short) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_japanese)
```

```{r}
dat_inaug_corpus_sent_dplot_japanese <- textplot_dependencyparser(dat_inaug_corpus_sent_japanese, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_japanese
```

##### [**Russian**]{.smallcaps}

```{r}
# parse text
dat_udhr_corpus_russian <- dat_udhr_corpus[[12]]
dat_udhr_corpus_russian_short <- str_extract(dat_udhr_corpus_russian, "Принимая во внимание, что признание достоинства, присущего всем\nчленам человеческой семьи, и равных и неотъемлемых прав их является основой свободы, справедливости и всеобщего мира;")
dat_inaug_corpus_sent_russian <- udpipe_annotate(m_russian, x = dat_udhr_corpus_russian_short) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_russian)
```

```{r}
dat_inaug_corpus_sent_dplot_russian <- textplot_dependencyparser(dat_inaug_corpus_sent_russian, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_russian
```

##### [**Vietnamese**]{.smallcaps}

```{r}
# parse text
dat_inaug_corpus_sent_vietnamese <- udpipe_annotate(m_vietnamese, x = dat_udhr_corpus[[13]]) %>%
  as.data.frame()
# inspect
head(dat_inaug_corpus_sent_vietnamese)
```

```{r}
dat_inaug_corpus_sent_vietnamese_short <- dat_inaug_corpus_sent_vietnamese[1:26,]
dat_inaug_corpus_sent_dplot_vietnamese <- textplot_dependencyparser(dat_inaug_corpus_sent_vietnamese_short, size = 3) 
# show plot
dat_inaug_corpus_sent_dplot_vietnamese
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

##### [**English**]{.smallcaps}

```{r}
dat_udhr_corpus_english <- dat_udhr_corpus[[4]]
dat_udhr_corpus_english_dfmat <- dfm(tokens(dat_udhr_corpus_english, remove_punct = TRUE))
dat_udhr_corpus_english_dfmat_trim <- dfm_trim(dat_udhr_corpus_english_dfmat, min_termfreq = 10)

topfeatures_dat_udhr_corpus_english <- topfeatures(dat_udhr_corpus_english_dfmat_trim)
topfeatures_dat_udhr_corpus_english
nfeat(dat_udhr_corpus_english_dfmat_trim)
```

##### [**French**]{.smallcaps}

```{r}
dat_udhr_corpus_french <- dat_udhr_corpus[[5]]
dat_udhr_corpus_french_dfmat <- dfm(tokens(dat_udhr_corpus_french, remove_punct = TRUE))
dat_udhr_corpus_french_dfmat_trim <- dfm_trim(dat_udhr_corpus_french_dfmat, min_termfreq = 10)

topfeatures_dat_udhr_corpus_french <- topfeatures(dat_udhr_corpus_french_dfmat_trim)
topfeatures_dat_udhr_corpus_french
nfeat(dat_udhr_corpus_french_dfmat_trim)
```

#### Features co-occurrences

##### [**English**]{.smallcaps}

```{r}
dat_udhr_corpus_english_fcmat <- fcm(dat_udhr_corpus_english_dfmat_trim)
dat_udhr_corpus_english_fcmat
```

##### [**French**]{.smallcaps}

```{r}
dat_udhr_corpus_french_fcmat <- fcm(dat_udhr_corpus_french_dfmat_trim, context = "document")
dat_udhr_corpus_french_fcmat
```

You can test it on the other languages

# Twitter data

We the twitter.json data accessed from [here](https://github.com/quanteda/tutorials.quanteda.io/blob/master/content/data/twitter.json).
This is a JSON file (.json) downloaded from the Twitter stream API.

## Importing data

```{r}
dat_twitter <- readtext("data/twitter.json", source = "twitter")
```


### Create a corpus

```{r}
dat_twitter_corpus <- corpus(dat_twitter)
print(dat_twitter_corpus)
```

### Summary

```{r}
summary(dat_twitter_corpus, 10)
```

### Accessing parts of corpus

```{r}
dat_twitter_corpus[,1]
```

### Document-level information

```{r}
head(docvars(dat_twitter_corpus))

```

### Unique variable names (for volume)

```{r}
unique(docvars(dat_twitter_corpus, field = "lang"))

```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
dat_twitter_corpus_tok <- tokens(dat_twitter_corpus)
dat_twitter_corpus_tok
```

#### Without punctuations

```{r}
dat_twitter_corpus_tok_no_punct <- tokens(dat_twitter_corpus, remove_punct = TRUE)
dat_twitter_corpus_tok_no_punct
```

### Compound words

#### `kwic` Phrase

```{r}
dat_twitter_corpus_tok_no_punct_phrase <- kwic(dat_twitter_corpus_tok_no_punct, pattern =  phrase("the tory"), window = 6)
head(dat_twitter_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

```{r}
dat_twitter_corpus_tok_no_punct_comp <- tokens_compound(dat_twitter_corpus_tok_no_punct, pattern = phrase("the tory"))
dat_twitter_corpus_tok_no_punct_comp_kwic <- kwic(dat_twitter_corpus_tok_no_punct_comp, pattern = phrase("the_tory"))
head(dat_twitter_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
dat_twitter_corpus_tok_no_punct_ngram <- tokens_ngrams(dat_twitter_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(dat_twitter_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(dat_twitter_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
dat_twitter_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(dat_twitter_corpus_tok_no_punct, n = 2:4, skip = 1:2) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(dat_twitter_corpus_tok_no_punct_ngram_skip, 10)

# Last 10 rows
tail(dat_twitter_corpus_tok_no_punct_ngram_skip, 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_dat_twitter <- dictionary(list(Tories = c("Tory*", "Conservative*"),
                        Labour = c("Socialist*", "Labour*")))
print(dict_dat_twitter)
```

#### Token lookup

```{r}
dat_twitter_corpus_tok_no_punct_dict_toks <- tokens_lookup(dat_twitter_corpus_tok_no_punct, dictionary = dict_dat_twitter)
print(dat_twitter_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(dat_twitter_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

#### Download and load language model

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

```{r}
file_to_check <- "models/english-ewt-ud-2.5-191206.udpipe"

if (file.exists(file = file_to_check)){
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}else{
  m_english <- udpipe_download_model(model_dir = "models/", language = "english-ewt")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}
```

#### Tokenise, tag, dependency parsing

We use the already tokenised text, with no punctuations.

```{r}
dat_twitter_anndf <- udpipe_annotate(m_english, x = dat_twitter_corpus_tok_no_punct[[2]]) %>%
  as.data.frame() 
# inspect
head(dat_twitter_anndf, 10)
```

#### Dependency parsing

```{r}
# parse text
dat_twitter_corpus_sent <- udpipe_annotate(m_english, x = dat_twitter_corpus[[2]]) %>%
  as.data.frame()
# inspect
head(dat_twitter_corpus_sent)
```

```{r}
dat_twitter_corpus_sent_short <- dat_twitter_corpus_sent[3:21,]
dat_twitter_corpus_sent_dplot <- textplot_dependencyparser(dat_twitter_corpus_sent_short, size = 3) 
# show plot
dat_twitter_corpus_sent_dplot
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

```{r}
dat_twitter_corpus_dfmat <- dfm(dat_twitter_corpus_tok_no_punct)
dat_twitter_corpus_dfmat_trim <- dfm_trim(dat_twitter_corpus_dfmat, min_termfreq = 50)

topfeatures_dat_twitter_corpus <- topfeatures(dat_twitter_corpus_dfmat_trim)
topfeatures_dat_twitter_corpus
nfeat(dat_twitter_corpus_dfmat_trim)
```

#### Features co-occurrences

```{r}
dat_twitter_corpus_fcmat <- fcm(dat_twitter_corpus_dfmat_trim)
dat_twitter_corpus_fcmat
```

# Single web page

## Read_html

```{r}
web_page <- rvest::read_html("https://www.tidyverse.org/packages/")
web_page
```

Because the downloaded file contains a unnecessary information.
We process the data to extract only the text from the webpage.

## Extract headline

```{r}
header_web_page <- web_page %>%
  # extract paragraphs
  rvest::html_nodes("h1") %>%
  # extract text
  rvest::html_text() 
head(header_web_page)
```

## Extract text

```{r}
web_page_txt <- web_page %>%
  # extract paragraphs
  rvest::html_nodes("p") %>%
  # extract text
  rvest::html_text()
head(web_page_txt)
```


## Create a corpus

```{r}
web_page_txt_corpus <- corpus(web_page_txt)
print(web_page_txt_corpus)
```

### Summary

```{r}
summary(web_page_txt_corpus, 10)
```

### Accessing parts of corpus

```{r}
web_page_txt_corpus[[4]]
```

### Document-level information

```{r}
head(docvars(web_page_txt_corpus))

```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
web_page_txt_corpus_tok <- tokens(web_page_txt_corpus)
web_page_txt_corpus_tok
```

#### Without punctuations

```{r}
web_page_txt_corpus_tok_no_punct <- tokens(web_page_txt_corpus, remove_punct = TRUE)
web_page_txt_corpus_tok_no_punct
```

### Compound words

#### `kwic` Phrase

```{r}
web_page_txt_corpus_tok_no_punct_phrase <- kwic(web_page_txt_corpus_tok_no_punct, pattern =  phrase("the tidy*"), window = 6)
head(web_page_txt_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

```{r}
web_page_txt_corpus_tok_no_punct_comp <- tokens_compound(web_page_txt_corpus_tok_no_punct, pattern = phrase("the tidy*"))
web_page_txt_corpus_tok_no_punct_comp_kwic <- kwic(web_page_txt_corpus_tok_no_punct_comp, pattern = phrase("the_tidy*"))
head(web_page_txt_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
web_page_txt_corpus_tok_no_punct_ngram <- tokens_ngrams(web_page_txt_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(web_page_txt_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(web_page_txt_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
web_page_txt_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(web_page_txt_corpus_tok_no_punct, n = 2:4, skip = 1:2)
# Top 10 rows
head(web_page_txt_corpus_tok_no_punct_ngram_skip[[1]], 10)

# Last 10 rows
tail(web_page_txt_corpus_tok_no_punct_ngram_skip[[1]], 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_web_page_txt <- dictionary(list(tidy_family = c("tidy*", "ggplot**"),
                        r_packages = "*r"))
print(dict_web_page_txt)
```

#### Token lookup

```{r}
web_page_txt_corpus_tok_no_punct_dict_toks <- tokens_lookup(web_page_txt_corpus_tok_no_punct, dictionary = dict_web_page_txt)
print(web_page_txt_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(web_page_txt_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

#### Download and load language model

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

```{r}
file_to_check <- "models/english-ewt-ud-2.5-191206.udpipe"

if (file.exists(file = file_to_check)){
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}else{
  m_english <- udpipe_download_model(model_dir = "models/", language = "english-ewt")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}
```

#### Tokenise, tag, dependency parsing

We use the already tokenised text, with no punctuations.

```{r}
web_page_txt_anndf <- udpipe_annotate(m_english, x = web_page_txt_corpus_tok_no_punct[[1]]) %>%
  as.data.frame() 
# inspect
head(web_page_txt_anndf, 10)
```

#### Dependency parsing

```{r}
# parse text
web_page_txt_corpus_sent <- udpipe_annotate(m_english, x = web_page_txt_corpus[[1]]) %>%
  as.data.frame()
# inspect
head(web_page_txt_corpus_sent)
```

```{r}
web_page_txt_corpus_sent_dplot <- textplot_dependencyparser(web_page_txt_corpus_sent, size = 3) 
# show plot
web_page_txt_corpus_sent_dplot
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

```{r}
web_page_txt_corpus_dfmat <- dfm(web_page_txt_corpus_tok_no_punct)
web_page_txt_corpus_dfmat_trim <- dfm_trim(web_page_txt_corpus_dfmat, min_termfreq = 5)

topfeatures_web_page_txt_corpus <- topfeatures(web_page_txt_corpus_dfmat_trim)
topfeatures_web_page_txt_corpus
nfeat(web_page_txt_corpus_dfmat_trim)
```

#### Features co-occurrences

```{r}
web_page_txt_corpus_fcmat <- fcm(web_page_txt_corpus_dfmat_trim)
web_page_txt_corpus_fcmat
```

# Multiple webpages

## Read_html

```{r}
website <- "https://www.tidyverse.org/packages/" %>% 
  rvest::read_html()
website
```

```{r}
a_elements <- website %>% 
  rvest::html_elements(css = "div.package > a")
a_elements
```

## Extract headline

```{r}
links <- a_elements %>%
  rvest::html_attr(name = "href")
links
```

## Extract subpages

```{r}
pages <- links %>% 
  map(rvest::read_html)
pages
```

The structure seems to be similar across all pages

```{r}
pages %>% 
  map(rvest::html_element, css = "a.navbar-brand") %>% 
  map_chr(rvest::html_text)

```

and extracting version number

```{r}
pages %>% 
  map(rvest::html_element, css = "small.nav-text.text-muted.me-auto") %>% 
  map_chr(rvest::html_text)
```

and we can also add all into a tibble

## Extract text

```{r}
pages_table <- tibble(
  name = pages %>% 
    map(rvest::html_element, css = "a.navbar-brand") %>% 
    map_chr(rvest::html_text),
  version = pages %>% 
    map(rvest::html_element, css = "small.nav-text.text-muted.me-auto") %>% 
    map_chr(rvest::html_text),
  CRAN = pages %>% 
    map(rvest::html_element, css = "ul.list-unstyled > li:nth-child(1) > a") %>% 
    map_chr(rvest::html_attr, name = "href"),
  Learn = pages %>% 
    map(rvest::html_element, css = "ul.list-unstyled > li:nth-child(4) > a") %>% 
    map_chr(rvest::html_attr, name = "href"), 
  text = pages %>%
    map(rvest::html_element,  css = "body") %>%
    map_chr(rvest::html_text2)
)
pages_table
```

## Create a corpus

```{r}
web_pages_txt_corpus <- corpus(pages_table)
print(web_pages_txt_corpus)
```

### Summary

```{r}
summary(web_pages_txt_corpus, 10)
```

### Accessing parts of corpus

```{r}
web_pages_txt_corpus[[4]]
```

### Document-level information

```{r}
head(docvars(web_pages_txt_corpus))

```

## Advanced manipulations

### Tokens

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries.
We can remove punctuations or not

#### With punctuations

```{r}
web_pages_txt_corpus_tok <- tokens(web_pages_txt_corpus)
web_pages_txt_corpus_tok
```

#### Without punctuations

```{r}
web_pages_txt_corpus_tok_no_punct <- tokens(web_pages_txt_corpus, remove_punct = TRUE)
web_pages_txt_corpus_tok_no_punct
```

### Compound words

#### `kwic` Phrase

```{r}
web_pages_txt_corpus_tok_no_punct_phrase <- kwic(web_pages_txt_corpus_tok_no_punct, pattern =  phrase("the tidy*"), window = 6)
head(web_pages_txt_corpus_tok_no_punct_phrase, 10)
```

#### Compounds

```{r}
web_pages_txt_corpus_tok_no_punct_comp <- tokens_compound(web_pages_txt_corpus_tok_no_punct, pattern = phrase("the tidy*"))
web_pages_txt_corpus_tok_no_punct_comp_kwic <- kwic(web_pages_txt_corpus_tok_no_punct_comp, pattern = phrase("the_tidy*"))
head(web_pages_txt_corpus_tok_no_punct_comp_kwic, 10)
```

### N-grams

N-grams are a subfamily of compound words.
They can be named as "bi-grams", "tri-grams", etc.
N-grams yield a sequence of tokens from already tokenised text object.

#### Multi-grams

The code below allows to obtain the sequences of consecutive compound words, with 2, 3 or 4 compound words.

```{r}
web_pages_txt_corpus_tok_no_punct_ngram <- tokens_ngrams(web_pages_txt_corpus_tok_no_punct, n = 2:4) %>% 
  unlist() %>%
  tolower() %>%
  table()
# Top 10 rows
head(web_pages_txt_corpus_tok_no_punct_ngram, 10)

# Last 10 rows
tail(web_pages_txt_corpus_tok_no_punct_ngram, 10)
```

#### Skip-grams

Skip-grams allow to obtain non consecutive n-grams

```{r}
web_pages_txt_corpus_tok_no_punct_ngram_skip <- tokens_ngrams(web_pages_txt_corpus_tok_no_punct, n = 2:4, skip = 1:2)
# Top 10 rows
head(web_pages_txt_corpus_tok_no_punct_ngram_skip[[7]], 10)

# Last 10 rows
tail(web_pages_txt_corpus_tok_no_punct_ngram_skip[[7]], 10)
```

### Dictionary

If you have a dictionary with various words that fall within a generic word (e.g., variants of pronunciation of a word), then you can look these up.
Here, we will create a dictionary that we populate ourselves and we show how to use it to search for items

#### Create dictionary

```{r}
dict_web_pages_txt <- dictionary(list(tidy_family = c("tidy*", "ggplot**"),
                        r_packages = "*r"))
print(dict_web_pages_txt)
```

#### Token lookup

```{r}
web_pages_txt_corpus_tok_no_punct_dict_toks <- tokens_lookup(web_pages_txt_corpus_tok_no_punct, dictionary = dict_web_pages_txt)
print(web_pages_txt_corpus_tok_no_punct_dict_toks)
```

#### DFM

```{r}
dfm(web_pages_txt_corpus_tok_no_punct_dict_toks)
```

### Part of Speech tagging

Part-of-Speech tagging (or PoS-Tagging) is used to distinguish different part of speech, e.g., the sentence: "Jane likes the girl" can be tagged as "Jane/NNP likes/VBZ the/DT girl/NN", where NNP = proper noun (singular), VBZ = 3rd person singular present tense verb, DT = determiner, and NN = noun (singular or mass).
We will use the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package

#### Download and load language model

Before using the PoS-tagger, we need to download a language model.
As you can see from typing `?udpipe_download_model`, there are 65 languages trained on 101 treebanks from [here](https://universaldependencies.org/)

```{r}
file_to_check <- "models/english-ewt-ud-2.5-191206.udpipe"

if (file.exists(file = file_to_check)){
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}else{
  m_english <- udpipe_download_model(model_dir = "models/", language = "english-ewt")
  m_english <- udpipe_load_model(file = "models/english-ewt-ud-2.5-191206.udpipe")
}
```

#### Tokenise, tag, dependency parsing

We use the already tokenised text, with no punctuations.

```{r}
web_pages_txt_anndf <- udpipe_annotate(m_english, x = web_pages_txt_corpus_tok_no_punct[[1]]) %>%
  as.data.frame() 
# inspect
head(web_pages_txt_anndf, 10)
```

#### Dependency parsing

```{r}
# parse text
web_pages_txt_corpus_sent <- udpipe_annotate(m_english, x = web_pages_txt_corpus[[1]]) %>%
  as.data.frame()
# inspect
head(web_pages_txt_corpus_sent)
```

```{r}
web_pages_txt_corpus_sent_dplot <- textplot_dependencyparser(web_pages_txt_corpus_sent, size = 3) 
# show plot
web_pages_txt_corpus_sent_dplot
```

### Feature co-occurrence matrix (FCM)

Feature co-occurrence matrix (FCM) records the number of co-occurrences of tokens

#### Computing number of co-occurrences

```{r}
web_pages_txt_corpus_dfmat <- dfm(web_pages_txt_corpus_tok_no_punct)
web_pages_txt_corpus_dfmat_trim <- dfm_trim(web_pages_txt_corpus_dfmat, min_termfreq = 5)

topfeatures_web_pages_txt_corpus <- topfeatures(web_pages_txt_corpus_dfmat_trim)
topfeatures_web_pages_txt_corpus
nfeat(web_pages_txt_corpus_dfmat_trim)
```

#### Features co-occurrences

```{r}
web_pages_txt_corpus_fcmat <- fcm(web_pages_txt_corpus_dfmat_trim)
web_pages_txt_corpus_fcmat
```

# session info

```{r}
sessionInfo()
```
